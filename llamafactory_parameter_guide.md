# LLaMA Factory å‚æ•°è®¾ç½®å®Œå…¨æŒ‡å—ï¼šç”Ÿäº§çŽ¯å¢ƒæœ€ä½³å®žè·µ

## å‰è¨€

LLaMA Factory ä½œä¸ºå½“å‰æœ€å—æ¬¢è¿Žçš„å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒæ¡†æž¶ï¼Œå…¶å‚æ•°é…ç½®çš„å¤æ‚æ€§ç»å¸¸è®©åˆå­¦è€…æœ›è€Œå´æ­¥ã€‚æœ¬æŒ‡å—åŸºäºŽ[å®˜æ–¹æ–‡æ¡£](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/arguments.html#id2)ï¼Œæä¾›ç”Ÿäº§çŽ¯å¢ƒä¸‹çš„å‚æ•°è®¾ç½®æœ€ä½³å®žè·µï¼Œå¸®åŠ©æ‚¨ä»Žé›¶å¼€å§‹æŽŒæ¡ä¸“ä¸šçº§çš„æ¨¡åž‹å¾®è°ƒæŠ€èƒ½ã€‚

## LLaMA Factory æž¶æž„æ¦‚è§ˆ

LLaMA Factory æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒæ¡†æž¶ï¼Œæ”¯æŒ 100+ ç§æ¨¡åž‹çš„é«˜æ•ˆå¾®è°ƒï¼Œæä¾›é›¶ä»£ç çš„ CLI å’Œ Web UI ç•Œé¢ã€‚

### æ ¸å¿ƒç‰¹æ€§
- **æ¨¡åž‹æ”¯æŒå¹¿æ³›**ï¼šæ”¯æŒ LLaMAã€Qwenã€ChatGLMã€Baichuan ç­‰ä¸»æµæ¨¡åž‹
- **æ–¹æ³•å¤šæ ·**ï¼šæ”¯æŒå…¨å‚æ•°å¾®è°ƒã€LoRAã€QLoRA ç­‰å¤šç§å¾®è°ƒæ–¹å¼
- **ç®—æ³•å…ˆè¿›**ï¼šé›†æˆ FlashAttentionã€DeepSpeedã€GaLore ç­‰åŠ é€ŸæŠ€æœ¯
- **ç”Ÿäº§å°±ç»ª**ï¼šæä¾›å®Œæ•´çš„ç›‘æŽ§ã€è¯„ä¼°å’Œéƒ¨ç½²èƒ½åŠ›

## å‚æ•°ä½“ç³»æž¶æž„

### å‚æ•°åˆ†ç±»æ¦‚è§ˆ

æ ¹æ®LLaMA Factoryå®˜æ–¹æ–‡æ¡£ï¼Œå‚æ•°ä½“ç³»åˆ†ä¸º8å¤§ç±»åˆ«ï¼Œæ¯ç±»å‚æ•°æ‰¿æ‹…ä¸åŒçš„åŠŸèƒ½èŒè´£ï¼š

### å…³é”®å‚æ•°ä¾èµ–å…³ç³»

ä¸Šå›¾å±•ç¤ºäº†LLaMA Factoryä¸­æ ¸å¿ƒå‚æ•°çš„ä¾èµ–å…³ç³»ï¼Œçº¢è‰²èŠ‚ç‚¹ä¸ºç”¨æˆ·å¯æŽ§çš„è¾“å…¥å‚æ•°ï¼Œç»¿è‰²èŠ‚ç‚¹ä¸ºæœ€ç»ˆçš„è¾“å‡ºæ•ˆæžœã€‚

## ä¸€ã€å¾®è°ƒå‚æ•°è¯¦è§£

å¾®è°ƒå‚æ•°ï¼ˆFinetuningArgumentsï¼‰æ˜¯LLaMA Factoryçš„æ ¸å¿ƒé…ç½®ï¼Œå†³å®šäº†å¾®è°ƒçš„æ–¹æ³•å’Œæ•ˆæžœã€‚

### 1.1 åŸºæœ¬å‚æ•°

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `stage` | str | "sft" | æ ¹æ®ä»»åŠ¡é€‰æ‹© | è®­ç»ƒé˜¶æ®µï¼špt(é¢„è®­ç»ƒ)ã€sft(ç›‘ç£å¾®è°ƒ)ã€rm(å¥–åŠ±æ¨¡åž‹)ã€ppoã€dpoã€kto |
| `finetuning_type` | str | "lora" | ä¼˜å…ˆé€‰æ‹©lora | å¾®è°ƒæ–¹æ³•ï¼šloraã€freezeã€full |
| `pure_bf16` | bool | False | å»ºè®®å¼€å¯ | çº¯bf16è®­ç»ƒï¼Œæå‡æ€§èƒ½ |
| `plot_loss` | bool | False | ç”Ÿäº§çŽ¯å¢ƒå¼€å¯ | ä¿å­˜lossæ›²çº¿ç”¨äºŽåˆ†æž |

**ç”Ÿäº§çŽ¯å¢ƒé…ç½®å»ºè®®**ï¼š
```yaml
stage: sft
finetuning_type: lora
pure_bf16: true
plot_loss: true
compute_accuracy: true  # è¯„ä¼°æ—¶è®¡ç®—å‡†ç¡®çŽ‡
```

### 1.2 LoRAå‚æ•°è¯¦è§£

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯æœ€å¸¸ç”¨çš„é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼š

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `lora_rank` | int | 8 | 8-32 | LoRAçŸ©é˜µçš„ç§©ï¼Œå½±å“è¡¨è¾¾èƒ½åŠ› |
| `lora_alpha` | int | None | rankÃ—2 | ç¼©æ”¾ç³»æ•°ï¼Œä¸€èˆ¬ä¸ºrankçš„2å€ |
| `lora_dropout` | float | 0 | 0-0.1 | DropoutçŽ‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| `lora_target` | str | "all" | "all" | åº”ç”¨LoRAçš„æ¨¡å— |
| `use_rslora` | bool | False | å¤§rankæ—¶å¼€å¯ | ç§©ç¨³å®šLoRAï¼Œæå‡è®­ç»ƒç¨³å®šæ€§ |
| `use_dora` | bool | False | å¤æ‚ä»»åŠ¡è€ƒè™‘ | æƒé‡åˆ†è§£LoRAï¼Œæå‡è¡¨è¾¾èƒ½åŠ› |

**ç”Ÿäº§çŽ¯å¢ƒé…ç½®ç­–ç•¥**ï¼š
```yaml
# åŸºç¡€é…ç½®ï¼ˆæŽ¨èï¼‰
lora_rank: 16
lora_alpha: 32
lora_dropout: 0
lora_target: all

# é«˜æ€§èƒ½é…ç½®ï¼ˆæ˜¾å­˜å……è¶³æ—¶ï¼‰
lora_rank: 64
lora_alpha: 128
use_rslora: true
```

### 1.3 RLHFå‚æ•°

ç”¨äºŽäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ŒåŒ…æ‹¬DPOã€PPOã€KTOç­‰æ–¹æ³•ï¼š

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `pref_beta` | float | 0.1 | 0.1-0.5 | åå¥½æŸå¤±betaå‚æ•° |
| `pref_loss` | str | "sigmoid" | "sigmoid" | åå¥½æŸå¤±ç±»åž‹ |
| `dpo_label_smoothing` | float | 0.0 | 0.0-0.1 | DPOæ ‡ç­¾å¹³æ»‘ |

### 1.4 é«˜çº§ä¼˜åŒ–ç®—æ³•

LLaMA Factoryæ”¯æŒå¤šç§å‰æ²¿ä¼˜åŒ–ç®—æ³•ï¼š

**GaLoreå‚æ•°**ï¼š
```yaml
use_galore: true
galore_rank: 16
galore_update_interval: 200
galore_scale: 0.25
```

**BAdamå‚æ•°**ï¼š
```yaml
use_badam: true
badam_mode: "layer"
badam_switch_interval: 50
```

**Apolloå‚æ•°**ï¼š
```yaml
use_apollo: true
apollo_rank: 16
apollo_update_interval: 200
```

## äºŒã€æ•°æ®å‚æ•°è¯¦è§£

æ•°æ®å‚æ•°æŽ§åˆ¶æ•°æ®é›†çš„åŠ è½½ã€å¤„ç†å’Œå¢žå¼ºç­–ç•¥ã€‚

### 2.1 åŸºç¡€æ•°æ®é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `dataset` | str | None | å¿…å¡« | è®­ç»ƒæ•°æ®é›†åç§° |
| `template` | str | None | æ¨¡åž‹å¯¹åº”æ¨¡æ¿ | promptæ¨¡æ¿ |
| `cutoff_len` | int | 2048 | æ ¹æ®æ•°æ®åˆ†æž | æœ€å¤§åºåˆ—é•¿åº¦ |
| `train_on_prompt` | bool | False | çœ‹ä»»åŠ¡éœ€æ±‚ | æ˜¯å¦åœ¨promptä¸Šè®­ç»ƒ |
| `mask_history` | bool | False | å¯¹è¯ä»»åŠ¡å»ºè®®true | æ˜¯å¦åªåœ¨å½“å‰è½®è®­ç»ƒ |

### 2.2 æ•°æ®å¤„ç†é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `streaming` | bool | False | å¤§æ•°æ®é›†å¼€å¯ | æµå¼æ•°æ®åŠ è½½ |
| `packing` | bool | None | é¢„è®­ç»ƒå¼€å¯ | åºåˆ—æ‰“åŒ…ï¼Œæå‡æ•ˆçŽ‡ |
| `mix_strategy` | str | "concat" | æ ¹æ®éœ€æ±‚é€‰æ‹© | å¤šæ•°æ®é›†æ··åˆç­–ç•¥ |
| `preprocessing_num_workers` | int | None | è®¾ä¸ºCPUæ ¸æ•° | é¢„å¤„ç†å¹¶è¡Œåº¦ |

**ç”Ÿäº§çŽ¯å¢ƒé…ç½®ç¤ºä¾‹**ï¼š
```yaml
# å¯¹è¯ä»»åŠ¡é…ç½®
dataset: your_chat_dataset
template: qwen
cutoff_len: 4096
mask_history: true
preprocessing_num_workers: 16

# é¢„è®­ç»ƒä»»åŠ¡é…ç½®
dataset: your_pretrain_dataset
streaming: true
packing: true
cutoff_len: 2048
```

## ä¸‰ã€æ¨¡åž‹å‚æ•°è¯¦è§£

æ¨¡åž‹å‚æ•°æŽ§åˆ¶æ¨¡åž‹çš„åŠ è½½ã€é‡åŒ–å’ŒæŽ¨ç†é…ç½®ã€‚

### 3.1 åŸºç¡€æ¨¡åž‹é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `model_name_or_path` | str | None | å¿…å¡« | æ¨¡åž‹è·¯å¾„ |
| `cache_dir` | str | None | è®¾ç½®æœ¬åœ°ç¼“å­˜ | æ¨¡åž‹ç¼“å­˜ç›®å½• |
| `trust_remote_code` | bool | False | è‡ªå®šä¹‰æ¨¡åž‹éœ€è¦ | æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç  |
| `model_revision` | str | "main" | æŒ‡å®šç‰ˆæœ¬ | æ¨¡åž‹ç‰ˆæœ¬ |

### 3.2 é‡åŒ–é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `quantization_bit` | int | None | 4æˆ–8 | é‡åŒ–ä½æ•° |
| `quantization_type` | str | "nf4" | "nf4" | é‡åŒ–ç±»åž‹ |
| `double_quantization` | bool | True | True | åŒé‡é‡åŒ– |

### 3.3 æ€§èƒ½ä¼˜åŒ–

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `flash_attn` | str | "auto" | "fa2" | FlashAttentionç‰ˆæœ¬ |
| `enable_liger_kernel` | bool | False | å»ºè®®å¼€å¯ | Ligerå†…æ ¸ä¼˜åŒ– |
| `use_unsloth` | bool | False | LoRAæ—¶è€ƒè™‘ | UnslothåŠ é€Ÿ |

**ç”Ÿäº§çŽ¯å¢ƒé…ç½®ç¤ºä¾‹**ï¼š
```yaml
# é«˜æ€§èƒ½é…ç½®
model_name_or_path: /path/to/model
flash_attn: fa2
enable_liger_kernel: true
trust_remote_code: true

# é‡åŒ–é…ç½®ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶ï¼‰
quantization_bit: 4
quantization_type: nf4
double_quantization: true
```

## å››ã€è®­ç»ƒå‚æ•°è¯¦è§£

è®­ç»ƒå‚æ•°æŽ§åˆ¶æ¨¡åž‹è®­ç»ƒçš„æ ¸å¿ƒè¶…å‚æ•°å’Œä¼˜åŒ–ç­–ç•¥ã€‚

### 4.1 æ ¸å¿ƒè®­ç»ƒå‚æ•°

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `learning_rate` | float | 5e-5 | 5e-5åˆ°1e-4 | å­¦ä¹ çŽ‡ï¼Œå½±å“è®­ç»ƒç¨³å®šæ€§ |
| `num_train_epochs` | float | 3.0 | 2-5 | è®­ç»ƒè½®æ•° |
| `per_device_train_batch_size` | int | 1 | 1-4 | å•è®¾å¤‡æ‰¹é‡å¤§å° |
| `gradient_accumulation_steps` | int | 1 | 4-16 | æ¢¯åº¦ç´¯ç§¯æ­¥æ•° |
| `max_grad_norm` | float | 1.0 | 1.0 | æ¢¯åº¦è£å‰ª |
| `warmup_steps` | int | 0 | æ€»æ­¥æ•°çš„10% | é¢„çƒ­æ­¥æ•° |

### 4.2 ä¼˜åŒ–å™¨é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `optim` | str | "adamw_torch" | "adamw_torch" | ä¼˜åŒ–å™¨ç±»åž‹ |
| `lr_scheduler_type` | str | "linear" | "cosine" | å­¦ä¹ çŽ‡è°ƒåº¦å™¨ |
| `weight_decay` | float | 0.0 | 0.01 | æƒé‡è¡°å‡ |

### 4.3 åˆ†å¸ƒå¼è®­ç»ƒ

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `deepspeed` | str | None | stage3é…ç½® | DeepSpeedé…ç½®æ–‡ä»¶ |
| `ddp_timeout` | int | 1800 | é€‚å½“å¢žåŠ  | DDPè¶…æ—¶æ—¶é—´ |

**ç”Ÿäº§çŽ¯å¢ƒè®­ç»ƒé…ç½®**ï¼š
```yaml
# åŸºç¡€é…ç½®
learning_rate: 5e-05
num_train_epochs: 3.0
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
max_grad_norm: 1.0
warmup_steps: 100

# ä¼˜åŒ–å™¨é…ç½®
optim: adamw_torch
lr_scheduler_type: cosine
weight_decay: 0.01

# åˆ†å¸ƒå¼é…ç½®ï¼ˆå¤šå¡æ—¶ï¼‰
deepspeed: configs/ds_z3_config.json
ddp_timeout: 180000000
```

## äº”ã€ç”Ÿæˆå‚æ•°è¯¦è§£

ç”Ÿæˆå‚æ•°æŽ§åˆ¶æ¨¡åž‹æŽ¨ç†æ—¶çš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥ã€‚

### 5.1 è§£ç ç­–ç•¥

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `do_sample` | bool | True | True | æ˜¯å¦ä½¿ç”¨é‡‡æ · |
| `temperature` | float | 0.95 | 0.7-1.0 | æ¸©åº¦å‚æ•°ï¼ŒæŽ§åˆ¶éšæœºæ€§ |
| `top_p` | float | 0.7 | 0.8-0.95 | æ ¸é‡‡æ ·å‚æ•° |
| `top_k` | int | 50 | 50-100 | Top-Ké‡‡æ · |
| `num_beams` | int | 1 | 1 | æŸæœç´¢å®½åº¦ |

### 5.2 é•¿åº¦æŽ§åˆ¶

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `max_length` | int | 1024 | æ ¹æ®éœ€æ±‚ | æœ€å¤§ç”Ÿæˆé•¿åº¦ |
| `max_new_tokens` | int | 1024 | 512-2048 | æœ€å¤§æ–°ç”Ÿæˆtokenæ•° |
| `repetition_penalty` | float | 1.0 | 1.0-1.1 | é‡å¤æƒ©ç½š |

**ç”Ÿäº§çŽ¯å¢ƒç”Ÿæˆé…ç½®**ï¼š
```yaml
# å¹³è¡¡è´¨é‡å’Œåˆ›é€ æ€§
do_sample: true
temperature: 0.8
top_p: 0.9
top_k: 50
max_new_tokens: 2048
repetition_penalty: 1.05
```

## å…­ã€è¯„ä¼°å‚æ•°è¯¦è§£

è¯„ä¼°å‚æ•°ç”¨äºŽæ¨¡åž‹æ€§èƒ½è¯„æµ‹ã€‚

### 6.1 è¯„ä¼°ä»»åŠ¡é…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `task` | str | None | æ ¹æ®éœ€æ±‚é€‰æ‹© | è¯„ä¼°ä»»åŠ¡ç±»åž‹ |
| `batch_size` | int | 4 | 8-16 | è¯„ä¼°æ‰¹é‡å¤§å° |
| `n_shot` | int | 5 | 5 | Few-shotç¤ºä¾‹æ•° |
| `lang` | str | "en" | "zh"æˆ–"en" | è¯„ä¼°è¯­è¨€ |

**æ”¯æŒçš„è¯„ä¼°ä»»åŠ¡**ï¼š
- `mmlu_test`: MMLUè‹±æ–‡è¯„æµ‹
- `ceval_validation`: C-Evalä¸­æ–‡è¯„æµ‹  
- `cmmlu_test`: CMMLUä¸­æ–‡è¯„æµ‹

## ä¸ƒã€ç›‘æŽ§å‚æ•°è¯¦è§£

ç›‘æŽ§å‚æ•°ç”¨äºŽå®žéªŒè·Ÿè¸ªå’Œå¯è§†åŒ–ã€‚

### 7.1 SwanLabé…ç½®

| å‚æ•°åç§° | ç±»åž‹ | é»˜è®¤å€¼ | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® | è¯´æ˜Ž |
|---------|------|--------|-------------|------|
| `use_swanlab` | bool | False | å»ºè®®å¼€å¯ | æ˜¯å¦ä½¿ç”¨SwanLab |
| `swanlab_project` | str | "llamafactory" | é¡¹ç›®åç§° | SwanLabé¡¹ç›®å |
| `swanlab_mode` | str | "cloud" | "cloud" | è¿è¡Œæ¨¡å¼ |

### 7.2 å…¶ä»–ç›‘æŽ§å·¥å…·

- **WandB**: è®¾ç½®çŽ¯å¢ƒå˜é‡ `WANDB_PROJECT`
- **TensorBoard**: è‡ªåŠ¨ç”Ÿæˆlogsç›®å½•
- **MLflow**: æ”¯æŒå®žéªŒè·Ÿè¸ª

## å…«ã€çŽ¯å¢ƒå˜é‡è¯¦è§£

çŽ¯å¢ƒå˜é‡æä¾›å…¨å±€é…ç½®èƒ½åŠ›ã€‚

### 8.1 ç¡¬ä»¶æŽ§åˆ¶

| å˜é‡å | è¯´æ˜Ž | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® |
|--------|------|-------------|
| `CUDA_VISIBLE_DEVICES` | GPUè®¾å¤‡é€‰æ‹© | "0,1,2,3" |
| `ASCEND_RT_VISIBLE_DEVICES` | NPUè®¾å¤‡é€‰æ‹© | "0,1,2,3" |

### 8.2 åˆ†å¸ƒå¼é…ç½®

| å˜é‡å | è¯´æ˜Ž | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® |
|--------|------|-------------|
| `MASTER_ADDR` | ä¸»èŠ‚ç‚¹åœ°å€ | å®žé™…IPåœ°å€ |
| `MASTER_PORT` | ä¸»èŠ‚ç‚¹ç«¯å£ | 29500 |
| `NPROC_PER_NODE` | æ¯èŠ‚ç‚¹GPUæ•° | å®žé™…GPUæ•°é‡ |

### 8.3 è°ƒè¯•é…ç½®

| å˜é‡å | è¯´æ˜Ž | ç”Ÿäº§çŽ¯å¢ƒå»ºè®® |
|--------|------|-------------|
| `LLAMAFACTORY_VERBOSITY` | æ—¥å¿—çº§åˆ« | "INFO" |
| `WANDB_DISABLED` | ç¦ç”¨wandb | æ ¹æ®éœ€æ±‚ |

## ä¹ã€æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥

### 9.1 æ˜¾å­˜ä¼˜åŒ–å†³ç­–æµç¨‹

ä¸Šå›¾å±•ç¤ºäº†ç³»ç»Ÿæ€§çš„æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ï¼Œä»Žè¯„ä¼°éœ€æ±‚åˆ°æœ€ç»ˆæˆåŠŸè®­ç»ƒçš„å®Œæ•´æµç¨‹ã€‚

### 9.2 æ˜¾å­˜ä¼°ç®—å…¬å¼

**åŸºç¡€æ˜¾å­˜è®¡ç®—**ï¼š
```
æ€»æ˜¾å­˜ = æ¨¡åž‹æƒé‡ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼ + æ¡†æž¶å¼€é”€
```

**å„ç»„ä»¶è¯¦ç»†ä¼°ç®—**ï¼š

| ç»„ä»¶ | è®¡ç®—å…¬å¼ | 7Bæ¨¡åž‹ç¤ºä¾‹ |
|------|----------|-----------|
| æ¨¡åž‹æƒé‡ | å‚æ•°é‡ Ã— ç²¾åº¦å­—èŠ‚æ•° | 7B Ã— 2 = 14GB (BF16) |
| ä¼˜åŒ–å™¨çŠ¶æ€ | æ¨¡åž‹æƒé‡ Ã— 2 (Adam) | 14GB Ã— 2 = 28GB |
| æ¿€æ´»å€¼ | batch_size Ã— seq_len Ã— 2.5GB/1K | åŠ¨æ€å˜åŒ– |
| æ¡†æž¶å¼€é”€ | ~1-2GB | 1.5GB |

### 9.3 åˆ†çº§ä¼˜åŒ–ç­–ç•¥

#### Level 1: åŸºç¡€ä¼˜åŒ–ï¼ˆæ— æŸæ€§èƒ½ï¼‰
```yaml
# å¯ç”¨é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶
flash_attn: fa2
enable_liger_kernel: true

# ä½¿ç”¨æ··åˆç²¾åº¦
bf16: true
pure_bf16: true

# ä¼˜åŒ–æ•°æ®åŠ è½½
dataloader_num_workers: 4
dataloader_pin_memory: true
```

#### Level 2: ä¸­åº¦ä¼˜åŒ–ï¼ˆè½»å¾®æ€§èƒ½å½±å“ï¼‰
```yaml
# å‡å°‘æ‰¹é‡å¤§å°ï¼Œå¢žåŠ æ¢¯åº¦ç´¯ç§¯
per_device_train_batch_size: 1
gradient_accumulation_steps: 16

# æ¢¯åº¦æ£€æŸ¥ç‚¹
gradient_checkpointing: true

# ä¼˜åŒ–å™¨çŠ¶æ€å¸è½½
optim_target_modules: ["gate_proj", "up_proj"]
```

#### Level 3: æ·±åº¦ä¼˜åŒ–ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
```yaml
# 4bité‡åŒ–
quantization_bit: 4
quantization_type: nf4
double_quantization: true

# ä½¿ç”¨QLORA
finetuning_type: lora
lora_rank: 16
```

#### Level 4: æžè‡´ä¼˜åŒ–ï¼ˆåˆ†å¸ƒå¼ï¼‰
```yaml
# DeepSpeed ZeRO Stage 3
deepspeed: configs/ds_z3_config.json

# CPUå¸è½½
deepspeed_config:
  zero_optimization:
    stage: 3
    cpu_offload: true
    cpu_offload_params: true
```

### 9.4 æ˜¾å­˜ç›‘æŽ§å·¥å…·

**å®žæ—¶ç›‘æŽ§å‘½ä»¤**ï¼š
```bash
# GPUæ˜¾å­˜ç›‘æŽ§
watch -n 1 nvidia-smi

# è¯¦ç»†å†…å­˜åˆ†æž
python -c "
import torch
print('GPU Count:', torch.cuda.device_count())
print('Current GPU:', torch.cuda.current_device())
print('GPU Memory:', torch.cuda.get_device_properties(0).total_memory/1024**3, 'GB')
"
```

**PyTorchæ˜¾å­˜åˆ†æž**ï¼š
```python
import torch

def print_memory_usage():
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
            memory_cached = torch.cuda.memory_reserved(i) / 1024**3
            print(f"GPU {i}: Allocated {memory_allocated:.2f}GB, Cached {memory_cached:.2f}GB")
```

## åã€ç”Ÿäº§çŽ¯å¢ƒæœ€ä½³å®žè·µ

### 10.1 é…ç½®æ¨¡æ¿é€‰æ‹©

æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„æ¨¡æ¿ï¼š

#### å•å¡24GBé…ç½®ï¼ˆRTX 4090ç­‰ï¼‰
```yaml
# åŸºç¡€é…ç½®
model_name_or_path: /path/to/7B-model
stage: sft
finetuning_type: lora
template: qwen

# æ˜¾å­˜ä¼˜åŒ–
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
cutoff_len: 2048
enable_liger_kernel: true
flash_attn: fa2

# LoRAé…ç½®
lora_rank: 16
lora_alpha: 32
lora_target: all

# è®­ç»ƒé…ç½®
learning_rate: 5e-05
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_steps: 100
```

#### åŒå¡48GBé…ç½®ï¼ˆA6000ç­‰ï¼‰
```yaml
# åŸºç¡€é…ç½®
model_name_or_path: /path/to/7B-model
stage: sft
finetuning_type: lora

# æ›´å¤§æ‰¹é‡å¤§å°
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
cutoff_len: 4096

# æ›´é«˜LoRA rank
lora_rank: 32
lora_alpha: 64

# åˆ†å¸ƒå¼é…ç½®
ddp_find_unused_parameters: false
dataloader_num_workers: 8
```

#### å¤šå¡80GBé…ç½®ï¼ˆA100ç­‰ï¼‰
```yaml
# é«˜æ€§èƒ½é…ç½®
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
cutoff_len: 8192

# é«˜çº§LoRA
lora_rank: 64
lora_alpha: 128
use_rslora: true

# æˆ–è€…å…¨å‚æ•°å¾®è°ƒ
finetuning_type: full
learning_rate: 1e-05
```

### 10.2 æ•°æ®é¢„å¤„ç†æœ€ä½³å®žè·µ

#### æ•°æ®è´¨é‡æŽ§åˆ¶
```yaml
# æ•°æ®è¿‡æ»¤
max_samples: 50000
val_size: 0.1

# åºåˆ—é•¿åº¦åˆ†æž
cutoff_len: 4096  # åŸºäºŽP99åˆ†ä½æ•°è®¾ç½®
train_on_prompt: false
mask_history: true  # å¯¹è¯ä»»åŠ¡

# å¤šæ•°æ®é›†æ··åˆ
dataset: dataset1,dataset2,dataset3
mix_strategy: interleave_under
interleave_probs: 0.5,0.3,0.2
```

#### é¢„å¤„ç†ä¼˜åŒ–
```yaml
# å¹¶è¡Œå¤„ç†
preprocessing_num_workers: 16
overwrite_cache: false

# æµå¼åŠ è½½ï¼ˆå¤§æ•°æ®é›†ï¼‰
streaming: true
buffer_size: 16384

# åºåˆ—æ‰“åŒ…ï¼ˆé¢„è®­ç»ƒï¼‰
packing: true
neat_packing: true
```

### 10.3 è®­ç»ƒç›‘æŽ§é…ç½®

#### æŸå¤±æ›²çº¿ç›‘æŽ§
```yaml
# åŸºç¡€ç›‘æŽ§
plot_loss: true
logging_steps: 10
save_steps: 500
eval_steps: 500

# é«˜çº§ç›‘æŽ§
use_swanlab: true
swanlab_project: my-llama-project
swanlab_mode: cloud

# è¯„ä¼°é…ç½®
eval_strategy: steps
per_device_eval_batch_size: 2
eval_on_each_dataset: true
```

#### æ£€æŸ¥ç‚¹ç®¡ç†
```yaml
# ä¿å­˜ç­–ç•¥
output_dir: ./checkpoints
save_total_limit: 3
save_strategy: steps
save_steps: 1000

# æ–­ç‚¹ç»­è®­
resume_from_checkpoint: ./checkpoints/checkpoint-1000
```

### 10.4 æŽ¨ç†éƒ¨ç½²é…ç½®

#### APIæœåŠ¡é…ç½®
```bash
# çŽ¯å¢ƒå˜é‡è®¾ç½®
export API_PORT=8000
export MAX_CONCURRENT=4
export API_KEY=your-secret-key

# å¯åŠ¨APIæœåŠ¡
llamafactory-cli api \
    --model_name_or_path ./merged_model \
    --template qwen \
    --infer_backend vllm
```

#### vLLMé…ç½®
```yaml
# vLLMæŽ¨ç†ä¼˜åŒ–
infer_backend: vllm
vllm_maxlen: 4096
vllm_gpu_util: 0.9
vllm_enforce_eager: false
vllm_max_lora_rank: 64
```

### 10.5 æ€§èƒ½è°ƒä¼˜æŒ‡å—

#### è®­ç»ƒé€Ÿåº¦ä¼˜åŒ–
1. **æ•°æ®åŠ è½½ä¼˜åŒ–**ï¼š
   - å¢žåŠ  `dataloader_num_workers`
   - å¯ç”¨ `dataloader_pin_memory`
   - ä½¿ç”¨ `streaming` æ¨¡å¼

2. **è®¡ç®—ä¼˜åŒ–**ï¼š
   - å¯ç”¨ `flash_attn: fa2`
   - ä½¿ç”¨ `enable_liger_kernel`
   - å¼€å¯ `pure_bf16`

3. **é€šä¿¡ä¼˜åŒ–**ï¼ˆå¤šå¡ï¼‰ï¼š
   - è®¾ç½® `ddp_find_unused_parameters: false`
   - è°ƒæ•´ `ddp_timeout`
   - ä½¿ç”¨é«˜é€Ÿç½‘ç»œè¿žæŽ¥

#### å†…å­˜ä¼˜åŒ–ä¼˜å…ˆçº§
1. `enable_liger_kernel: true` ï¼ˆé¦–é€‰ï¼Œæ€§èƒ½å½±å“æœ€å°ï¼‰
2. å‡å°‘ `per_device_train_batch_size`
3. å¯ç”¨ `gradient_checkpointing`
4. ä½¿ç”¨é‡åŒ– `quantization_bit: 4`
5. DeepSpeed ZeRO Stage 3

### 10.6 é”™è¯¯æŽ’æŸ¥æŒ‡å—

#### å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

**CUDA Out of Memory (OOM)**ï¼š
```bash
# è§£å†³æ­¥éª¤
1. é™ä½Ž per_device_train_batch_size åˆ° 1
2. å¯ç”¨ enable_liger_kernel: true
3. å‡å°‘ cutoff_len
4. ä½¿ç”¨ quantization_bit: 4
5. ä½¿ç”¨ DeepSpeed
```

**è®­ç»ƒé€Ÿåº¦æ…¢**ï¼š
```bash
# ä¼˜åŒ–æ­¥éª¤
1. æ£€æŸ¥ dataloader_num_workers è®¾ç½®
2. å¯ç”¨ flash_attn: fa2
3. ä½¿ç”¨ enable_liger_kernel: true
4. æ£€æŸ¥ç½‘ç»œå¸¦å®½ï¼ˆå¤šå¡è®­ç»ƒï¼‰
```

**Lossä¸æ”¶æ•›**ï¼š
```bash
# è°ƒè¯•æ­¥éª¤
1. æ£€æŸ¥å­¦ä¹ çŽ‡è®¾ç½®ï¼ˆ5e-5æ˜¯å¥½çš„èµ·ç‚¹ï¼‰
2. ç¡®è®¤æ•°æ®æ ¼å¼æ­£ç¡®
3. æ£€æŸ¥ mask_history è®¾ç½®
4. è°ƒæ•´ warmup_steps
```

## åä¸€ã€å®Œæ•´é…ç½®ç¤ºä¾‹

### 11.1 ç”Ÿäº§çº§å¯¹è¯æ¨¡åž‹å¾®è°ƒ

**å®Œæ•´çš„é…ç½®æ–‡ä»¶ç¤ºä¾‹** (`configs/production_chat.yaml`):

```yaml
# ===== åŸºç¡€é…ç½® =====
model_name_or_path: /models/Qwen2.5-7B-Instruct
stage: sft
finetuning_type: lora
template: qwen

# ===== æ•°æ®é…ç½® =====
dataset: alpaca_zh,belle_multiturn
dataset_dir: ./data
cutoff_len: 4096
val_size: 0.1
mix_strategy: interleave_under
interleave_probs: 0.7,0.3
preprocessing_num_workers: 16
max_samples: 50000

# ===== è®­ç»ƒé…ç½® =====
learning_rate: 5e-05
num_train_epochs: 3.0
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
max_grad_norm: 1.0
lr_scheduler_type: cosine
warmup_steps: 200
weight_decay: 0.01

# ===== LoRAé…ç½® =====
lora_rank: 32
lora_alpha: 64
lora_dropout: 0.05
lora_target: all
use_rslora: true

# ===== ä¼˜åŒ–é…ç½® =====
bf16: true
pure_bf16: true
flash_attn: fa2
enable_liger_kernel: true
dataloader_num_workers: 8
dataloader_pin_memory: true

# ===== ç›‘æŽ§é…ç½® =====
output_dir: ./outputs/production_chat
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3
plot_loss: true

# ===== è¯„ä¼°é…ç½® =====
eval_strategy: steps
per_device_eval_batch_size: 4
eval_on_each_dataset: true
compute_accuracy: true

# ===== ç›‘æŽ§å·¥å…· =====
use_swanlab: true
swanlab_project: production-chat-model
swanlab_mode: cloud

# ===== å…¶ä»–é…ç½® =====
trust_remote_code: true
seed: 42
```

### 11.2 å‘½ä»¤è¡Œå¯åŠ¨æ–¹å¼

```bash
# å•å¡è®­ç»ƒ
llamafactory-cli train configs/production_chat.yaml

# å¤šå¡è®­ç»ƒ (2å¡)
torchrun --nproc_per_node=2 --master_port=29500 \
    -m llamafactory.train configs/production_chat.yaml

# å¤šèŠ‚ç‚¹è®­ç»ƒ (æ¯èŠ‚ç‚¹4å¡ï¼Œ2èŠ‚ç‚¹)
torchrun --nnodes=2 --node_rank=0 --nproc_per_node=4 \
    --master_addr=10.0.0.1 --master_port=29500 \
    -m llamafactory.train configs/production_chat.yaml
```

### 11.3 Dockeréƒ¨ç½²é…ç½®

**Dockerfile**:
```dockerfile
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# å®‰è£…Pythonå’Œä¾èµ–
RUN apt-get update && apt-get install -y python3 python3-pip git
RUN pip3 install llamafactory[torch,metrics]

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace
COPY configs/ ./configs/
COPY data/ ./data/

# è®¾ç½®çŽ¯å¢ƒå˜é‡
ENV CUDA_VISIBLE_DEVICES=0,1,2,3
ENV LLAMAFACTORY_VERBOSITY=INFO

# å¯åŠ¨å‘½ä»¤
CMD ["llamafactory-cli", "train", "configs/production_chat.yaml"]
```

**docker-compose.yml**:
```yaml
version: '3.8'
services:
  llama-factory:
    build: .
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0,1,2,3
    volumes:
      - ./data:/workspace/data
      - ./outputs:/workspace/outputs
      - ./models:/workspace/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 4
              capabilities: [gpu]
```

### 11.4 APIæœåŠ¡éƒ¨ç½²

**å¯åŠ¨æŽ¨ç†API**:
```bash
# åŸºç¡€APIæœåŠ¡
llamafactory-cli api \
    --model_name_or_path ./outputs/production_chat \
    --template qwen \
    --port 8000

# é«˜æ€§èƒ½vLLM API
llamafactory-cli api \
    --model_name_or_path ./outputs/production_chat \
    --template qwen \
    --infer_backend vllm \
    --vllm_gpu_util 0.9 \
    --port 8000
```

**APIè°ƒç”¨ç¤ºä¾‹**:
```python
import requests

url = "http://localhost:8000/v1/chat/completions"
headers = {"Content-Type": "application/json"}

data = {
    "model": "default",
    "messages": [
        {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡åž‹"}
    ],
    "temperature": 0.8,
    "max_tokens": 2048
}

response = requests.post(url, json=data, headers=headers)
print(response.json())
```

## åäºŒã€å¿«é€Ÿå¼€å§‹æŒ‡å—

### 12.1 WebUI é›¶ä»£ç ä½¿ç”¨

å¯¹äºŽåˆå­¦è€…ï¼ŒæŽ¨èä½¿ç”¨WebUIè¿›è¡Œå¯è§†åŒ–é…ç½®ï¼š

```bash
# å¯åŠ¨WebUI
llamafactory-cli webui

# è®¿é—®åœ°å€ (é»˜è®¤)
http://localhost:7860
```

**WebUIç•Œé¢è¯´æ˜Ž**ï¼š
- **Train**ï¼šè®­ç»ƒé…ç½®ç•Œé¢ï¼Œè®¾ç½®æ‰€æœ‰è®­ç»ƒå‚æ•°
- **Evaluate & Predict**ï¼šè¯„ä¼°æ¨¡åž‹æ€§èƒ½
- **Chat**ï¼šä¸Žå¾®è°ƒåŽçš„æ¨¡åž‹å¯¹è¯æµ‹è¯•
- **Export**ï¼šå¯¼å‡ºæ¨¡åž‹ç”¨äºŽéƒ¨ç½²

### 12.2 å‘½ä»¤è¡Œå¿«é€Ÿå¼€å§‹

```bash
# 1. å‡†å¤‡æ•°æ®é›†
# å°†æ•°æ®æ”¾åœ¨ data/ ç›®å½•ä¸‹

# 2. åˆ›å»ºé…ç½®æ–‡ä»¶
cat > quick_start.yaml << EOF
model_name_or_path: /path/to/Qwen2.5-7B-Instruct
stage: sft
finetuning_type: lora
template: qwen
dataset: your_dataset
cutoff_len: 2048
learning_rate: 5e-05
num_train_epochs: 3
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
output_dir: ./outputs/quick_start
EOF

# 3. å¼€å§‹è®­ç»ƒ
llamafactory-cli train quick_start.yaml

# 4. æµ‹è¯•æ¨¡åž‹
llamafactory-cli chat \
    --model_name_or_path ./outputs/quick_start \
    --template qwen
```

### 12.3 å‚æ•°è°ƒä¼˜å·¥ä½œæµ

ä¸Šå›¾å±•ç¤ºäº†å®Œæ•´çš„æ¨¡åž‹å¾®è°ƒå·¥ä½œæµç¨‹ï¼Œä»Žæ•°æ®å‡†å¤‡åˆ°æœ€ç»ˆéƒ¨ç½²çš„æ¯ä¸ªå…³é”®æ­¥éª¤ã€‚

#### å·¥ä½œæµè¯¦ç»†è¯´æ˜Ž

**é˜¶æ®µä¸€ï¼šæ•°æ®å‡†å¤‡**
- æ•°æ®æ ¼å¼åŒ–ï¼šè½¬æ¢ä¸ºLLaMA Factoryæ”¯æŒçš„æ ¼å¼
- Tokené•¿åº¦åˆ†æžï¼šä½¿ç”¨å®˜æ–¹è„šæœ¬åˆ†æžæ•°æ®åˆ†å¸ƒ
- è´¨é‡æ£€æŸ¥ï¼šåŽ»é™¤é‡å¤ã€é”™è¯¯ã€ä¸å®Œæ•´çš„æ•°æ®

**é˜¶æ®µäºŒï¼šåŸºçº¿æµ‹è¯•**
- ä½¿ç”¨æœ€å°é…ç½®ç¡®ä¿èƒ½å¤Ÿæ­£å¸¸è®­ç»ƒ
- è®°å½•åŸºçº¿losså’Œè¯„ä¼°æŒ‡æ ‡
- éªŒè¯è®­ç»ƒæµç¨‹çš„æ­£ç¡®æ€§

**é˜¶æ®µä¸‰ï¼šå‚æ•°è°ƒä¼˜**
- å­¦ä¹ çŽ‡è°ƒä¼˜ï¼šä»Ž5e-5å¼€å§‹ï¼Œè§‚å¯Ÿlossæ›²çº¿
- æ‰¹é‡å¤§å°ä¼˜åŒ–ï¼šåœ¨æ˜¾å­˜å…è®¸èŒƒå›´å†…æœ€å¤§åŒ–
- LoRA rankè°ƒæ•´ï¼šæ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©8-64
- æ•°æ®å¢žå¼ºï¼šå¤šæ•°æ®é›†æ··åˆã€åºåˆ—æ‰“åŒ…ç­‰

**é˜¶æ®µå››ï¼šæ€§èƒ½è¯„ä¼°**
- Lossæ›²çº¿åˆ†æžï¼šç¡®ä¿æ”¶æ•›ä¸”æ— è¿‡æ‹Ÿåˆ
- éªŒè¯é›†è¯„ä¼°ï¼šè®¡ç®—å‡†ç¡®çŽ‡ç­‰å®¢è§‚æŒ‡æ ‡
- å¯¹è¯è´¨é‡æµ‹è¯•ï¼šäººå·¥è¯„ä¼°ç”Ÿæˆè´¨é‡

**é˜¶æ®µäº”ï¼šæ¨¡åž‹éƒ¨ç½²**
- æ¨¡åž‹å¯¼å‡ºï¼šåˆå¹¶LoRAæƒé‡
- APIéƒ¨ç½²ï¼šä½¿ç”¨vLLMç­‰é«˜æ€§èƒ½æŽ¨ç†å¼•æ“Ž
- æ€§èƒ½ç›‘æŽ§ï¼šç›‘æŽ§æŽ¨ç†å»¶è¿Ÿå’Œåžåé‡

## åä¸‰ã€æ€»ç»“ä¸Žå±•æœ›

### 13.1 æ ¸å¿ƒè¦ç‚¹å›žé¡¾

é€šè¿‡æœ¬æŒ‡å—çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥æŽŒæ¡ä»¥ä¸‹å…³é”®æŠ€èƒ½ï¼š

1. **å‚æ•°ä½“ç³»ç†è§£**ï¼šæŽŒæ¡8å¤§ç±»å‚æ•°çš„ä½œç”¨å’Œé…ç½®æ–¹æ³•
2. **æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥**ï¼šä»ŽåŸºç¡€ä¼˜åŒ–åˆ°æžè‡´ä¼˜åŒ–çš„å®Œæ•´æ–¹æ¡ˆ
3. **ç”Ÿäº§çŽ¯å¢ƒé…ç½®**ï¼šé’ˆå¯¹ä¸åŒç¡¬ä»¶çš„æœ€ä½³å®žè·µé…ç½®
4. **é—®é¢˜æŽ’æŸ¥èƒ½åŠ›**ï¼šå¿«é€Ÿå®šä½å’Œè§£å†³å¸¸è§è®­ç»ƒé—®é¢˜

### 13.2 å‚æ•°è®¾ç½®å†³ç­–æ ‘

```
é€‰æ‹©å¾®è°ƒæ–¹æ³• â†’ è¯„ä¼°ç¡¬ä»¶èµ„æº â†’ åˆ†æžæ•°æ®ç‰¹ç‚¹ â†’ è®¾ç½®åŸºç¡€å‚æ•° â†’ ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ â†’ ç›‘æŽ§è®­ç»ƒè¿‡ç¨‹ â†’ è°ƒä¼˜å…³é”®å‚æ•° â†’ è¯„ä¼°æ¨¡åž‹æ•ˆæžœ
```

### 13.3 æœ€ä½³å®žè·µç²¾è¦

#### ðŸŽ¯ å‚æ•°è®¾ç½®é»„é‡‘æ³•åˆ™
1. **å…ˆè·‘é€šï¼Œå†ä¼˜åŒ–**ï¼šç¡®ä¿åŸºç¡€é…ç½®èƒ½æ­£å¸¸è®­ç»ƒ
2. **ç›‘æŽ§ä¼˜å…ˆ**ï¼šå§‹ç»ˆå¼€å¯lossæ›²çº¿å’Œæ˜¾å­˜ç›‘æŽ§
3. **æ¸è¿›è°ƒä¼˜**ï¼šé€æ­¥è°ƒæ•´å‚æ•°ï¼Œé¿å…å¤§å¹…å˜åŠ¨
4. **éªŒè¯å¯¼å‘**ï¼šä»¥éªŒè¯é›†è¡¨çŽ°æŒ‡å¯¼å‚æ•°è°ƒæ•´

#### ðŸš€ æ€§èƒ½ä¼˜åŒ–æ¸…å•
- [ ] å¯ç”¨ `flash_attn: fa2`
- [ ] å¼€å¯ `enable_liger_kernel: true`
- [ ] ä½¿ç”¨ `pure_bf16: true`
- [ ] è®¾ç½®åˆé€‚çš„ `dataloader_num_workers`
- [ ] é…ç½® `gradient_accumulation_steps`

#### ðŸ›¡ï¸ ç¨³å®šæ€§ä¿éšœ
- [ ] è®¾ç½® `max_grad_norm: 1.0`
- [ ] é…ç½® `warmup_steps`
- [ ] ä½¿ç”¨ `lr_scheduler_type: cosine`
- [ ] å¯ç”¨ `plot_loss: true`
- [ ] è®¾ç½®åˆç†çš„ `val_size`

### 13.4 æŠ€æœ¯å‘å±•è¶‹åŠ¿

éšç€å¤§æ¨¡åž‹æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒLLaMA Factoryä¹Ÿåœ¨ä¸æ–­æ¼”è¿›ï¼š

**ç®—æ³•ä¼˜åŒ–æ–¹å‘**ï¼š
- æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¦‚Ring Attentionï¼‰
- æ–°çš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆå¦‚AdaLoRAã€QAdaLoRAï¼‰
- å†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚GradCacheã€Activation Checkpointingï¼‰

**å·¥ç¨‹ä¼˜åŒ–æ–¹å‘**ï¼š
- æ›´æ™ºèƒ½çš„è‡ªåŠ¨å‚æ•°è°ƒä¼˜
- æ›´å®Œå–„çš„åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ  
- æ›´ä¸°å¯Œçš„æ¨¡åž‹é‡åŒ–é€‰é¡¹

**ç”Ÿæ€å‘å±•æ–¹å‘**ï¼š
- ä¸Žæ›´å¤šæŽ¨ç†æ¡†æž¶çš„é›†æˆ
- æ›´å®Œå–„çš„æ¨¡åž‹è¯„ä¼°ä½“ç³»
- æ›´å¼ºå¤§çš„æ•°æ®å¤„ç†èƒ½åŠ›

### 13.5 å­¦ä¹ å»ºè®®

1. **åŠ¨æ‰‹å®žè·µ**ï¼šç†è®ºå­¦ä¹ åŽåŠ¡å¿…è¿›è¡Œå®žé™…æ“ä½œ
2. **å…³æ³¨ç¤¾åŒº**ï¼šè·Ÿè¿›LLaMA Factoryå®˜æ–¹æ›´æ–°å’Œç¤¾åŒºè®¨è®º
3. **è®°å½•ç»éªŒ**ï¼šå»ºç«‹è‡ªå·±çš„å‚æ•°é…ç½®çŸ¥è¯†åº“
4. **äº¤æµåˆ†äº«**ï¼šä¸ŽåŒè¡Œäº¤æµå¾®è°ƒç»éªŒå’ŒæŠ€å·§

### 13.6 å‚è€ƒèµ„æº

- **å®˜æ–¹æ–‡æ¡£**: https://llamafactory.readthedocs.io/
- **GitHubä»“åº“**: https://github.com/hiyouga/LLaMA-Factory
- **è®ºæ–‡å‚è€ƒ**: LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
- **ç¤¾åŒºè®ºå›**: GitHub Discussions å’Œç›¸å…³æŠ€æœ¯ç¾¤ç»„

## ç»“è¯­

å¤§è¯­è¨€æ¨¡åž‹å¾®è°ƒæ˜¯ä¸€é—¨æ—¢æœ‰ç†è®ºæ·±åº¦åˆé‡å®žè·µç»éªŒçš„æŠ€æœ¯ã€‚LLaMA Factoryä½œä¸ºå½“å‰æœ€ä¼˜ç§€çš„å¼€æºå¾®è°ƒæ¡†æž¶ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„å·¥å…·ã€‚

æŽŒæ¡å…¶å‚æ•°é…ç½®ä¸ä»…éœ€è¦ç†è§£åº•å±‚åŽŸç†ï¼Œæ›´éœ€è¦åœ¨å®žè·µä¸­ç§¯ç´¯ç»éªŒã€‚å¸Œæœ›æœ¬æŒ‡å—èƒ½æˆä¸ºæ‚¨çš„å¾—åŠ›åŠ©æ‰‹ï¼Œå¸®åŠ©æ‚¨åœ¨AIåº”ç”¨å¼€å‘çš„é“è·¯ä¸Šèµ°å¾—æ›´ç¨³ã€æ›´è¿œã€‚

è®°ä½ï¼š**å¥½çš„å‚æ•°é…ç½®æ˜¯æˆåŠŸå¾®è°ƒçš„ä¸€åŠï¼ŒæŒç»­çš„ä¼˜åŒ–å’Œç›‘æŽ§æ˜¯å¦ä¸€åŠ**ã€‚

æ„¿æ¯ä¸€ä½å¼€å‘è€…éƒ½èƒ½ç”¨æœ‰é™çš„ç®—åŠ›èµ„æºï¼Œåˆ›é€ å‡ºæ— é™çš„AIåº”ç”¨ä»·å€¼ã€‚

---

**ç‰ˆæƒå£°æ˜Ž**: æœ¬æŒ‡å—åŸºäºŽLLaMA Factoryå®˜æ–¹æ–‡æ¡£å’Œç¤¾åŒºæœ€ä½³å®žè·µæ•´ç†ï¼Œä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚

**ä½œè€…**: èµ„æ·±åŽç«¯å·¥ç¨‹å¸ˆï¼Œä¸“æ³¨äºŽå¤§æ¨¡åž‹å¾®è°ƒå’Œåˆ†å¸ƒå¼ç³»ç»Ÿï¼Œæ‹¥æœ‰20å¹´å¼€å‘ç»éªŒã€‚

**æ›´æ–°æ—¥æœŸ**: 2024å¹´12æœˆ

**æŠ€æœ¯äº¤æµ**: æ¬¢è¿Žé€šè¿‡GitHub Issueæˆ–æŠ€æœ¯ç¤¾åŒºäº¤æµå¾®è°ƒç»éªŒå’Œé—®é¢˜ã€‚ 